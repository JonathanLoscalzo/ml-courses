{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy\n",
    "You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (1): Vocab, Lexemes and StringStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared vocab and string store (2)\n",
    "- Vocab: stores data shared across multiple documents\n",
    "- To save memory, spaCy encodes all strings to hash values\n",
    "- Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "- String store: lookup table in both directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.strings.StringStore at 0x7f4f4da211e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f57e2679b63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoffee_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coffee'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoffee_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoffee_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcoffee_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoffee_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "coffee_hash, coffee_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', nlp.vocab.strings['coffee'])\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes\n",
    "Lexemes are context-independent entries in the vocabulary.\n",
    "\n",
    "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "\n",
    "Lexemes expose attributes, just like tokens.\n",
    "\n",
    "They hold context-independent information about a word, like the text, or whether the the word consists of alphabetic characters.\n",
    "\n",
    "Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A Lexeme object is an entry in the vocabulary\n",
    "# Contains the context-independent information about a word\n",
    "# Word text: lexeme.text and lexeme.orth (the hash)\n",
    "# Lexical attributes like lexeme.is_alpha\n",
    "# Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt asdf][./static/images/vocab_stringstore.png]\n",
    "\n",
    "The Doc contains words in context â€“ in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
    "\n",
    "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()# Create an n# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)lp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644858412616767388\n"
     ]
    }
   ],
   "source": [
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings['Bowie']\n",
    "print(bowie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look up the ID for 'Bowie' in the vocab\n",
    "#print(nlp_de.vocab.strings[bowie_id])\n",
    "#Why does this code throw an error?\n",
    "# Hashes canâ€™t be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures (2): Doc, Span and Token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Hello world, (Hello world,), ['GREETING'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "span_with_label, doc.ents, [ent.label_ for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "- Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "- Convert result to strings as late as possible\n",
    "- Use token attributes if available â€“ for example, token.i for the token index\n",
    "- Don't forget to pass in the shared vocab\n",
    "\n",
    "Let's practice!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "#___________________\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "print(pos_tags)\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if token.i + 1 < len(doc) and doc[token.i + 1].pos_ == \"VERB\":\n",
    "            result = token.text\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "## Word vectors and semantic similarity\n",
    "\n",
    "### Comparing semantic similarity\n",
    "- spaCy can compare two objects and predict similarity\n",
    "- Doc.similarity(), Span.similarity() and Token.similarity()\n",
    "- Take another object and return a similarity score (0 to 1)\n",
    "- Important: needs a model that has word vectors included, for example:  \n",
    "\n",
    "âœ… en_core_web_md (medium model)  \n",
    "âœ… en_core_web_lg (large model)  \n",
    "ðŸš« NOT en_core_web_sm (small model)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018373287411041\n",
      "pizza == pasta 0.32624283\n"
     ]
    }
   ],
   "source": [
    "# spacy.load('en_core_web_md')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1, \"==\", token2, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How does spaCy predict similarity?*\n",
    "- Similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representations of words\n",
    "- Generated using an algorithm like Word2Vec and lots of text\n",
    "- Can be added to spaCy's statistical models\n",
    "- Default: cosine similarity, but can be adjusted\n",
    "- Doc and Span vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9383564  -2.9524927   1.1866798   0.49744225 -0.11475766  0.804008\n",
      "  0.4672468  -1.1062207   2.9193573   1.800931   -0.31358248  1.1920271\n",
      " -1.2406584  -2.3237133   2.099902   -0.66673994 -0.96991694  0.8316833\n",
      "  0.10666084 -0.42245626  1.6402073   0.95437694  1.2855074  -2.038612\n",
      " -0.7317371  -0.17545497  0.14752543  1.327169    3.2502053  -3.9332502\n",
      "  1.7409098  -0.73711336  1.4852796  -2.8246899  -1.8938334  -1.2638527\n",
      "  5.298433   -1.2850044  -2.7470415  -1.5607052   5.181785    2.242096\n",
      " -2.1922808  -5.310454    1.0295098   1.484088   -1.5894104  -0.14745024\n",
      "  1.7829046   1.8879583   4.152973   -3.1493165  -0.18937713  2.09369\n",
      " -2.1269834   0.63290507  2.6979058   1.800016   -2.3953576   2.54901\n",
      "  1.0445759  -1.3137031   2.4631662  -0.07756937 -1.129545    0.1169464\n",
      "  1.3869805   0.53586185 -2.242661    2.8641388  -3.8719153  -0.6409143\n",
      "  0.6971829   4.484493   -1.6210997   2.494869    0.7218447  -3.3112261\n",
      " -0.2163549  -2.5339773  -1.1702836  -0.9627162  -3.7210062   1.5599163\n",
      " -1.8974628   1.4590962  -1.8888438   0.19177747 -1.6627836  -2.7840474\n",
      " -0.76189506 -0.2788751   1.743672   -1.8110999   1.5651491   1.7505348 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity depends on the application context\n",
    "- Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849750871465223\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.5189981   1.0769155  -2.4386053  -1.0038385  -0.09056883  3.2566302\n",
      "  1.1464092  -1.7798254   0.94800925  3.154657    1.5740933   1.4181501\n",
      " -0.6316143   0.6489268  -2.9598496  -3.2275555  -2.5892558   0.47790402\n",
      " -0.9010248  -0.08130313  1.0560243  -2.3348064  -2.1118944  -0.06434575\n",
      "  2.2199314   3.2613754  -0.10951382 -2.4045494  -1.9903516   0.31546313\n",
      "  0.34890515 -0.995184   -3.042478   -3.2609143   1.6715308  -1.5878627\n",
      "  4.6034174  -1.991154   -0.45828784 -1.1500666   9.190532    3.157382\n",
      "  1.064983   -1.8014977  -1.0308608  -0.79789555  0.17627871 -0.3339194\n",
      "  0.9729749   2.3049207  -3.8109362  -3.8562965   0.1393863  -0.5373318\n",
      " -3.0516777   0.18260963  2.7131486  -0.23883188 -2.8489332   0.93734485\n",
      " -1.8697594   4.6760097  -0.4455679   0.33904123  3.8083265  -0.60235274\n",
      "  1.409046   -2.2682066   1.1596813   0.8128674   1.7838109   0.44814005\n",
      " -0.62783265  2.7147424   0.922505   -0.02261603  0.06355977 -4.513331\n",
      "  0.05755246 -3.054604   -2.3484707   3.1645236  -3.7710278  -1.1659154\n",
      " -0.151761    1.1489285  -1.6033759   0.8907529  -2.915482    3.4167333\n",
      "  1.3270653  -0.10993195  2.310618   -3.511517    1.7025592   3.20171   ]\n"
     ]
    }
   ],
   "source": [
    "#__________________________________________________\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5361258385408284\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28746358\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(great restaurant, really nice bar, 0.616326)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "span1, span2, similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________\n",
    "### Combining models and rules\n",
    "#### Statistical models\n",
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
    "\n",
    "- Use cases: application needs to generalize based on examples\tâ €â €â €â €â €â €â €â €â €â €â €â € â €â €â €â €â €â €â €â €â €â €â €â € â €â €â €â €â €â €â €\n",
    "- Real-world examples: product names, person names, subject/object relationships\t\n",
    "- spaCy features: entity recognizer, dependency parser, part-of-speech tagger\n",
    "\n",
    "#### Rule-based systems\n",
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
    "\n",
    "\n",
    "- Use cases: dictionary with finite number of examples\n",
    "- Real-world examples: countries of the world, cities, drug names, dog breeds\n",
    "- spaCy features: tokenizer, Matcher, PhraseMatcher\n",
    "\n",
    "For complex tasks, itâ€™s usually better to train a statistical entity recognition model. However, statistical models require training data, so for many situations, rule-based approaches are more practical. This is especially true at the start of a project: you can use a rule-based approach as part of a data collection process, to help you â€œbootstrapâ€ a statistical model.\n",
    "\n",
    "[rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOVE_CATS love cats\n",
      "VERY_HAPPY very happy\n",
      "VERY_HAPPY very very happy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
    "matcher.add('LOVE_CATS', None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "matcher.add('VERY_HAPPY', None, pattern)\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matcher.vocab.strings[match_id], matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "# Adding statistical prediction\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficient phrase matching (1)\n",
    "- PhraseMatcher like regular expressions or keyword search â€“ but with access to the tokens!\n",
    "- Takes Doc object as patterns\n",
    "- More efficient and faster than the Matcher\n",
    "- Great for matching large word lists\n",
    "\n",
    "The phrase matcher is another helpful tool to find sequences of words in your data.\n",
    "\n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "It takes Doc objects as patterns.\n",
    "\n",
    "It's also really fast.\n",
    "\n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"IS_PUNCT\": True}, {\"LOWER\":\"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', '-', 'free', 'viewing']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"ad-free viewing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia, Argentina]\n",
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"static/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "print(patterns)\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia --> Namibia\n",
      "South --> South Africa\n",
      "Cambodia --> Cambodia\n",
      "Kuwait --> Kuwait\n",
      "Somalia --> Somalia\n",
      "Haiti --> Haiti\n",
      "Mozambique --> Mozambique\n",
      "Somalia --> Somalia\n",
      "Rwanda --> Rwanda\n",
      "Singapore --> Singapore\n",
      "Sierra --> Sierra Leone\n",
      "Afghanistan --> Afghanistan\n",
      "Iraq --> Iraq\n",
      "Sudan --> Sudan\n",
      "Congo --> Congo\n",
      "Haiti --> Haiti\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s use that country matcher on a longer text, analyze the syntax and update the documentâ€™s entities with the matched countries.\n",
    "\n",
    "# Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
    "# Overwrite the entities in doc.ents and add the matched span.\n",
    "# Get the matched spanâ€™s root head token.\n",
    "# Print the text of the head token and the span.\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"static/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"static/country_text.txt\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(TEXT)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    # The spanâ€™s root token is available as span.root. A tokenâ€™s head is available via the token.head attribute.\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitde7cbade5f3b409597a00bf77a4b8fa2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
